{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chest  cough  coughing  dry  fever  loss  months  on  pain  wt\n",
      "0      1      1         1    1      1     1       1   1     1   1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "text = [\"COUGH DRY 1.5 MONTHS CHEST PAIN ON COUGHING FEVER WT LOSS\"]\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix = vectorizer.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = ['COUGH DRY 1.5 MONTHS CHEST PAIN ON COUGHING FEVER WT LOSS']\n",
    "vectorizer.transform(text2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chest  chest pain  cough  cough dry  coughing  coughing fever  dry  \\\n",
      "0      1           1      1          1         1               1    1   \n",
      "\n",
      "   dry months  fever  fever wt  loss  months  months chest  on  on coughing  \\\n",
      "0           1      1         1     1       1             1   1            1   \n",
      "\n",
      "   pain  pain on  wt  wt loss  \n",
      "0     1        1   1        1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Implementation of the BOW model with n-gram:\n",
    "\n",
    "text = [\"COUGH DRY 1.5 MONTHS CHEST PAIN ON COUGHING FEVER WT LOSS\"]\n",
    "vectorizer = CountVectorizer(ngram_range = (1,2))\n",
    "count_matrix = vectorizer.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      chest     cough  coughing       dry     fever      loss    months  \\\n",
      "0  0.316228  0.316228  0.316228  0.316228  0.316228  0.316228  0.316228   \n",
      "\n",
      "         on      pain        wt  \n",
      "0  0.316228  0.316228  0.316228  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text = [\"COUGH DRY 1.5 MONTHS CHEST PAIN ON COUGHING FEVER WT LOSS\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "count_array = matrix.toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['altered', 'bowel', 'habits']]\n"
     ]
    }
   ],
   "source": [
    "# Cleaing the text\n",
    "import re\n",
    "import nltk\n",
    "text = 'ALTERED BOWEL HABITS'\n",
    "processed_article = text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "    print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cough'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\asad\\nlp-engine-for-symptoms\\feature-extraction.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m model1 \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mWord2Vec(data, min_count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \t\t\t\t\t\t\tvector_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m, window \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Print results\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m#model1.wv.similarity('Dry Cough', 'Cough Dry')\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m model1\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mevaluate_word_pairs(\u001b[39m'\u001b[39;49m\u001b[39mcough\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdry\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\models\\keyedvectors.py:1480\u001b[0m, in \u001b[0;36mKeyedVectors.evaluate_word_pairs\u001b[1;34m(self, pairs, delimiter, encoding, restrict_vocab, case_insensitive, dummy4unknown)\u001b[0m\n\u001b[0;32m   1478\u001b[0m original_key_to_index, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_to_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_to_index, ok_vocab\n\u001b[0;32m   1479\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(pairs, encoding\u001b[39m=\u001b[39;49mencoding) \u001b[39mas\u001b[39;00m fin:\n\u001b[0;32m   1481\u001b[0m         \u001b[39mfor\u001b[39;00m line_no, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(fin):\n\u001b[0;32m   1482\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line \u001b[39mor\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m#\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Ignore lines with comments.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\smart_open\\smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 188\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[0;32m    189\u001b[0m     uri,\n\u001b[0;32m    190\u001b[0m     mode,\n\u001b[0;32m    191\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    192\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[0;32m    193\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    194\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    195\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[0;32m    196\u001b[0m )\n\u001b[0;32m    197\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\smart_open\\smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m    359\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[1;32m--> 361\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39mbuffering, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cough'"
     ]
    }
   ],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    "\n",
    "# importing all necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Reads ‘alice.txt’ file\n",
    "# sample = open(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\alice.txt\", \"utf8\")\n",
    "# s = sample.read()\n",
    "\n",
    "s = 'COUGH DRY 1.5 MONTHS CHEST PAIN ON COUGHING FEVER WT LOSS'\n",
    "\n",
    "# Replaces escape character with space\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "\ttemp = []\n",
    "\t\n",
    "\t# tokenize the sentence into words\n",
    "\tfor j in word_tokenize(i):\n",
    "\t\ttemp.append(j.lower())\n",
    "\n",
    "\tdata.append(temp)\n",
    "\n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,\n",
    "\t\t\t\t\t\t\tvector_size = 100, window = 5)\n",
    "\n",
    "# Print results\n",
    "#model1.wv.similarity('Dry Cough', 'Cough Dry')\n",
    "\t\n",
    "model1.wv.evaluate_word_pairs('cough', 'dry')\n",
    "\n",
    "# # Create Skip Gram model\n",
    "# model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,\n",
    "# \t\t\t\t\t\t\t\t\t\t\twindow = 5, sg = 1)\n",
    "\n",
    "# # Print results\n",
    "# model2.wv.similarity('alice', 'wonderland')\n",
    "\t\n",
    "# model2.wv.similarity('alice', 'machines')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dry cough', 0.6184), ('coughing fever', 0.5968), ('pain coughing', 0.5871), ('chest pain', 0.5374), ('cough months', 0.4731)] \n",
      " [('coughing', 0.4487), ('cough', 0.3499), ('chest', 0.3193), ('fever', 0.2851), ('weight', 0.2416)] \n",
      " [('coughing fever weight', 0.7093), ('chest pain coughing', 0.7062), ('dry cough months', 0.6613), ('cough months chest', 0.6443), ('pain coughing fever', 0.6338)]\n",
      "dry cough\n",
      "coughing fever\n",
      "pain coughing\n",
      "chest pain\n",
      "cough months\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "dr_trans = 'DRY COUGH 1.5 MONTHS CHEST PAIN ON COUGHING FEVER WT LOSS'\n",
    "dr_trans = 'Patient is suffering from nausea and vomitting with chills and fever'\n",
    "dr_trans = 'LEFT SIDED NASAL OBSTRUCTION WITH THROAT DISCOMFORT'\n",
    "\n",
    "dr_trans = 'pain in hypogastrium radiates towards chest from 2 months'\n",
    "dr_trans = 'no h/o fever palpitation breathing difficulty and orthopnea'\n",
    "dr_trans = 'pain in hypogastrium radiates towards chest from 2 months'\n",
    "dr_trans = 'PAIN AND BLOOD IN URINE X 2 DAYS'\n",
    "dr_trans = 'headache fever with chills'\n",
    "dr_trans = 'A LARGE LOBULATED SOLID CYCTIC LESION IN THE MIDLINE &  AT THE INFRAUMBILICAL REGION INVOLVING THE ANTERIOR ABDOMINAL WALL AS DESCRIBED'\n",
    "dr_trans = 'Dry COUGH 1.5 MONTHS CHEST PAIN ON COUGHING FEVER Weight LOSS'\n",
    "\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(dr_trans, keyphrase_ngram_range=(1, 2))\n",
    "keywords2 = kw_model.extract_keywords(dr_trans, keyphrase_ngram_range=(2, 3))\n",
    "keywords1 = kw_model.extract_keywords(dr_trans, keyphrase_ngram_range=(0, 1))\n",
    "print(keywords, '\\n', keywords1, '\\n', keywords2)\n",
    "for i in range(0,len(keywords)):\n",
    "            print(keywords[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('redness', 0.7231), ('body', 0.4738)] \n",
      " [('body redness', 0.9136)] \n",
      " [('body redness', 0.9136), ('redness', 0.7231), ('body', 0.4738)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "from matplotlib.colors import PowerNorm\n",
    "#dr_trans = 'Paitent  suffering from nausea,  vomitting with chills Fever'\n",
    "dr_trans = 'neck pain with headache since 6 months'\n",
    "dr_trans = 'SENSITIVITY IN ALL TEETH'\n",
    "dr_trans = 'left knee joint pain since 3 years on and off'\n",
    "dr_trans = 'Anxiety Disorders and low mood since 1 month'\n",
    "\n",
    "\n",
    "problemName = 'Knee Joint Pain'\n",
    "#problemName = 'fever with chills and full body pain'\n",
    "\n",
    "problemName = 'follow up case of type 1 diabetes presented with DKA on 27.2.20 and discharged on 4.2.20 on LISPRO/GLARGINE'\n",
    "problemName = 'blurred or unstable vision'\n",
    "problemName = 'Dry COUGH 1.5 MONTHS CHEST PAIN ON COUGHING and fever Weight loss'\n",
    "problemName = 'Vitamin B1 Deficiency and hepatitis A1'\n",
    "problemName = 'Back pain LOW BACK PAIN FOR 1 MONTH'\n",
    "problemName = 'follow up case of type 1 diabetes presented with DKA on 27.2.20 and discharged on 4.2.20 on LISPRO/GLARGINE'\n",
    "problemName = 'Vitamin b12 and Hypervitaminosis A'\n",
    "problemName = 'Weight LOSS , Dry COUGH 1.5 MONTHS CHEST PAIN ON COUGHING'\n",
    "problemName = 'LOW BACK PAIN FOR 1 MONTH and body ache and t2dm'\n",
    "problemName = 'loss of hair'\n",
    "problemName = 'ALTERED BOWEL HABITS'\n",
    "problemName = 'Fracture Of The Olecranon'\n",
    "problemName = 'Full Body Redness'\n",
    "# Replaces escape character with space\n",
    "# f = dr_trans.replace(\"\\n\", \" \")\n",
    "\n",
    "# data = []\n",
    "\n",
    "# # iterate through each sentence in the file\n",
    "# for i in sent_tokenize(f):\n",
    "# \ttemp = []\n",
    "\t\n",
    "# \t# tokenize the sentence into words\n",
    "# \tfor j in word_tokenize(i):\n",
    "# \t\ttemp.append(j.lower())\n",
    "\n",
    "# \tdata.append(temp)\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "problemName_keyword = kw_model.extract_keywords(problemName, keyphrase_ngram_range=(2,2))\n",
    "keywords = kw_model.extract_keywords(problemName, keyphrase_ngram_range=(0, 1))\n",
    "problemName_keyword3 = kw_model.extract_keywords(problemName, keyphrase_ngram_range=(1,3))\n",
    "print(keywords ,'\\n' ,  problemName_keyword, '\\n', problemName_keyword3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hair\n",
      "loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hair', 'loss']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywordList = []\n",
    "for i in range(len(keywords)):\n",
    "    print(keywords[i][0])\n",
    "    keywordList.append(keywords[i][0])\n",
    "\n",
    "keywordList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss hair\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss hair']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywordList2 = []\n",
    "for i in range(len(problemName_keyword)):\n",
    "    print(problemName_keyword[i][0])\n",
    "    keywordList2.append(problemName_keyword[i][0])\n",
    "keywordList2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss hair\n",
      "hair loss\n",
      "['hair loss']\n"
     ]
    }
   ],
   "source": [
    "#sentence constructor like 'loss of hair' -> 'hair loss'\n",
    "def sentConstr(keywordList):\n",
    "    newKeywords = []\n",
    "    try:\n",
    "        for k in range(len(keywordList)):\n",
    "            print(keywordList[k+1]+' ' +keywordList[k])\n",
    "            newKeywords.append((keywordList[k+1]+' ' +keywordList[k]))\n",
    "            newKeywords.append((keywordList[k]+' ' +keywordList[k+1]))\n",
    "            print(keywordList[k]+ ' '+keywordList[k+1])\n",
    "            s_A = set(constructedKeyword)\n",
    "            s_B = set(keywordList2)\n",
    "            result = s_A-s_B\n",
    "            ls= list(result)\n",
    "            return ls\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "constructedKeyword = sentConstr(keywordList)\n",
    "print(constructedKeyword)\n",
    "    #newKeywords.append()\n",
    "# s_A = set(constructedKeyword)\n",
    "# s_B = set(keywordList2)\n",
    "# s_A,s_B\n",
    "# result = s_A-s_B\n",
    "# result\n",
    "# ls= list(result)\n",
    "#ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hair loss']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_A = set(constructedKeyword)\n",
    "s_B = set(keywordList2)\n",
    "s_A,s_B\n",
    "result = s_A-s_B\n",
    "result\n",
    "ls= list(result)\n",
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss hair\n",
      "hair loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss of hair', 'hair of loss']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence constructor like 'Vision Disturbances' -> 'Disturbances Of Vision'\n",
    "def sentConstr(keywordList):\n",
    "    newKeywords = []\n",
    "    try:\n",
    "        for k in range(len(keywordList)):\n",
    "            print(keywordList[k+1]+' ' +keywordList[k])\n",
    "            newKeywords.append((keywordList[k+1]+' '+ 'of'+ ' ' +keywordList[k]))\n",
    "            newKeywords.append((keywordList[k]+ ' ' + 'of' + ' ' +keywordList[k+1]))\n",
    "            print(keywordList[k]+ ' '+keywordList[k+1])\n",
    "            \n",
    "            return newKeywords\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "constructedKeyword = sentConstr(keywordList)\n",
    "#print(constructedKeyword)\n",
    "    #newKeywords.append()\n",
    "s_A = set(constructedKeyword)\n",
    "s_B = set(keywordList2)\n",
    "result = s_A-s_B\n",
    "ls= list(result)\n",
    "ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "def get_disease_list():\n",
    "    test_api =  'http://182.156.200.179:332/api/v1.0/Knowmed/getAllProblemList'\n",
    "    #payload = {\"alphabet\": \"a\"}\n",
    "    test_response = requests.post(test_api, json={\"alphabet\": \"\"})\n",
    "    response_data = test_response.json()\n",
    "\n",
    "\n",
    "\n",
    "    # Problem Names in list\n",
    "    problemNames = []\n",
    "    for i in range(len(response_data['responseValue'])):\n",
    "        #print(response_data['responseValue'][i]['problemName'])\n",
    "        problemNames.append(response_data['responseValue'][i]['problemName'].lower())\n",
    "    #print(problemNames)\n",
    "\n",
    "    disease_list = problemNames\n",
    "    #objects = cache.set(disease_list)\n",
    "    # disease_list = pd.read()\n",
    "    # disease_list['Disease Name'] = disease_list['Disease Name']\n",
    "    # disease_list['Disease Name'] = disease_list['Disease Name'].astype(str)\n",
    "    # disease_list['Disease Name'] = disease_list['Disease Name'].apply(lambda x:x.lower())\n",
    "    # disease_list = disease_list['Disease Name'].to_list()\n",
    "\n",
    "    return disease_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "bd1 = ['abdomen', 'abdomen right hypochondium', 'abdomen right lumber', 'abdomen right eliac region', 'abdomen left hypochondium', 'abdomen left lumber', 'abdomen left eliac region', 'abdomen epigastric region', 'abdomen umbilical region', 'abdomen hypogastrium', 'ankle', 'back', 'back lower', 'back upper', 'breast', 'buttock', 'calf', 'chest', 'ear', 'elbow', 'forehead', 'eye', 'face', 'finger', 'foot', 'hair', 'hand', 'head', 'heel', 'hip', 'knee', 'leg', 'lips', 'mouth', 'nail', 'neck', 'nose', 'palm', 'pelvis', 'shin', 'shoulder', 'skin', 'teeth', 'thigh', 'throat', 'thumb', 'toe', 'waist', 'wrist', 'full body', 'anywhere', 'vagina', 'penis', 'nipple', 'heart', 'kidney', 'lungs', 'liver', 'brain', 'bladder', 'stomach', 'intestines', 'uterus', 'blood', 'overy', 'testicles', 'limbs', 'spine', 'gollbladder', 'skull', 'gums', 'parotid gland', 'upper extremities', 'lower extremities', 'not specified', 'salivary glands', 'cartilagenous', 'multisystemic disorders', 'neurological disorder', 'bone', 'blood vessel', 'pancreas', 'adrenal glanel', 'lymphatic system', 'larynx', 'genital', 'cervix', 'bile duct', 'cns', 'prostate', 'respiratory system', 'skeletal system', 'renal system', 'cardivascular system', 'pitutiary gland', 'pneal gland', 'hypohealauous', 'parathyroid gland', 'thyroid gland', 'gastrointestinal system', 'anus', 'anal anal', 'oesophagus', 'rectum', 'cavernous sinus', 'tongue', 'paranasal sinus', 'hardplate', 'softplate', 'male reproductive system', 'female reproductive system', 'fallopian tubes', 'lower abdomen', 'adrenal glands', 'joint', 'multiple organ', 'gallbladder', 'muscle', 'muscle skeletal system', 'tissues', 'immune system  ', 'urinary tract infection', 'spinal cord', 'bone marrow', 'utreine corpus', 'testis', 'oral cavity', 'oropharynx', 'red blood cells', 'ans', 'hypothalamus', 'arteries', 'sole', 'trunk', 'labia majora', 'labia minora', 'glans penis', 'mouth angle', 'buccal mucosa', 'shaft penis', 'haematopoietic system', 'trachea', 'circulatory system', 'peripheral nervous system', 'depends on area of burn', 'vascular system', 'venous system', 'urinary bladder', 'mammary glands', 'reproductive system', 'urinary system']\n",
    "n = zip(bd1,bd1)\n",
    "d = dict(n)\n",
    "d\n",
    "with open(\"bodyOrgan.json\", \"w\") as outfile:\n",
    "    json.dump(d, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mm'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getBodyOrgan(w):\n",
    "    bodyOrganData = json.load(open('bodyOrgan.json'))\n",
    "    w = w.lower()\n",
    "    if w in bodyOrganData:\n",
    "        rt = bodyOrganData[w]\n",
    "        return rt\n",
    "    else:\n",
    "        return w\n",
    "o = 'mm'\n",
    "t = getBodyOrgan(o)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBodyOrgan(w):\n",
    "    bodyOrganData = json.load(open('bodyOrgan.json'))\n",
    "    w = w.lower()\n",
    "    if w in bodyOrganData:\n",
    "        rt = bodyOrganData[w]\n",
    "        return rt\n",
    "    else:\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disease_list():\n",
    "    test_api =  'http://182.156.200.179:332/api/v1.0/Knowmed/getAllProblemList'\n",
    "    #payload = {\"alphabet\": \"a\"}\n",
    "    test_response = requests.post(test_api, json={\"alphabet\": \"\"})\n",
    "    response_data = test_response.json()\n",
    "\n",
    "\n",
    "\n",
    "    # Problem Names in list\n",
    "    problemNames = []\n",
    "    for i in range(len(response_data['responseValue'])):\n",
    "        #print(response_data['responseValue'][i]['problemName'])\n",
    "        problemNames.append(response_data['responseValue'][i]['problemName'].lower())\n",
    "    #print(problemNames)\n",
    "\n",
    "    disease_list = problemNames\n",
    "    #objects = cache.set(disease_list)\n",
    "    # disease_list = pd.read()\n",
    "    # disease_list['Disease Name'] = disease_list['Disease Name']\n",
    "    # disease_list['Disease Name'] = disease_list['Disease Name'].astype(str)\n",
    "    # disease_list['Disease Name'] = disease_list['Disease Name'].apply(lambda x:x.lower())\n",
    "    # disease_list = disease_list['Disease Name'].to_list()\n",
    "\n",
    "    return disease_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Buildig sentences \n",
    "flatus = ['chills', 'with']  #\n",
    "\n",
    "symptoms = ['fever', 'vomiting'] #pro\n",
    "\n",
    "flatus = [ 'pait', 'saaf']\n",
    "symptoms = ['khana', 'bhookh']\n",
    "\n",
    "# Create an empty list\n",
    "keywords_list = []\n",
    "\n",
    "# Loop through products\n",
    "for symptom in symptoms:\n",
    "    # Loop through words\n",
    "    for flatu in flatus:\n",
    "        # Append combinations\n",
    "        keywords_list.append([symptom, symptom + ' ' + flatu ])\n",
    "        keywords_list.append([symptom, flatu + ' ' + symptom])\n",
    "        \n",
    "# Inspect keyword list\n",
    "from pprint import pprint\n",
    "pprint(keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found\n"
     ]
    }
   ],
   "source": [
    "from queue import Empty\n",
    "import re\n",
    "\n",
    "def vitamin(str1):\n",
    "\n",
    "  str = str1\n",
    "  str = str.lower()\n",
    "  match = re.findall(r'\\btype \\d diabetes\\b|\\bpain \\w* and upper back\\b|\\bdiabetes mellitus type \\d\\b|\\bglutaric aciduria type \\d\\b|\\bprimary hyperoxaluria type \\d\\b|\\bfull body \\w*\\b|\\bfull body \\w* \\w*\\b|\\b\\w* of the \\w*\\b|\\b\\w* of \\w*\\b|\\bback lower \\w*\\b|\\bback upper \\w*\\b|\\bback pain\\b|\\blump in \\w*\\b|\\blump on \\w*\\b|\\bitching on \\w*\\b|\\bchange in \\w* \\w*\\b|\\bchange in \\w*\\b|\\bchanges in \\w* \\w*\\b|\\bchanges in \\w*\\b|\\bswelling in \\w*\\b|\\bswelling on \\w*\\b|\\bvitamin d dependent rickets\\b|\\bpain in \\w* back\\b|\\bpain in mid back\\b|\\bbruise on \\w*\\b|\\bpain in back\\b|\\bpain in mid \\w*\\b|\\bpain \\w* and upper back\\b', str1)\n",
    "\n",
    "  return match\n",
    "\n",
    "str1 =  'PAIN NECK AND UPPER BACK SINCE 1 WEEK'\n",
    "match = vitamin(str1)\n",
    "if len(match) != 0 :\n",
    "  print(match)\n",
    "else:\n",
    "  print(\"not found\")\n",
    "# if len(match) != 0:\n",
    "#   for i in range(len(match)):\n",
    "#     print(match[i])\n",
    "\n",
    "#for i in match:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 =  'Vitamin b12 and Hypervitaminosis A deficiency'\n",
    "match = vitamin(str1)\n",
    "vit = 'vitamin'\n",
    "if vit in match:\n",
    "  print(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found ['type 1']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def type1():\n",
    "\n",
    "    str = 'follow up case of type 1 diabetes presented with DKA on 27.2.20 and discharged on 4.2.20 on LISPRO/GLARGINE'\n",
    "    str = str.lower()\n",
    "    match = re.findall(r'\\btype \\d\\b', str)\n",
    "\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12.5 months']\n"
     ]
    }
   ],
   "source": [
    "#find day month week and years\n",
    "import re\n",
    "def findDMW(dr_transcription):\n",
    "    str1 = dr_transcription\n",
    "    str1 = str1.lower()\n",
    "    # search() for letter word surrounded by space\n",
    "    # \\b is used to specify word boundary\n",
    "    result = re.findall(r\"\\b\\d. day\\b|\\b\\d week\\b|\\b\\d*\\.?\\d+ month\\b|\\b\\d+ days\\b|\\b\\d+ weeks\\b|\\b\\d*\\.?\\d+ months\\b|\\b\\d+day\\b|\\b\\d+days\\b|\\b\\d+week\\b|\\b\\d+weeks\\b|\\b\\d+month\\b|\\b\\d*\\.?\\d+months\\b|\\b\\d+ year\\b|\\b\\d+ years\\b|\\b\\d+ yrs\\b|\\b\\d+yrs\\b\", str1)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "txt = 'Dry COUGH 12.5 MONTHs CHEST PAIN ON COUGHING and fever Weight LOSS'\n",
    "\n",
    "d = findDMW(txt)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.5mg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find MG\n",
    "def findMg(dr_transcription):\n",
    "    str1 = dr_transcription\n",
    "    str1 = str1.lower()\n",
    "    result = re.findall(r\"\\b\\d*\\.?\\d+ mg\\b|\\b\\d*\\.?\\d+mg\\b\", str1)\n",
    "    return result\n",
    "\n",
    "txt = '10.5mg'\n",
    "d = findMg(txt)\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss of colour']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#Full Body Redness\n",
    "def findMg(dr_transcription):\n",
    "    str1 = dr_transcription\n",
    "    str1 = str1.lower()\n",
    "    result = re.findall(r\"\\bloss of \\w*\\b\", str1)\n",
    "    return result\n",
    "\n",
    "txt = 'Loss Of Colour Vision'\n",
    "d = findMg(txt)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find acute severe serious\n",
    "def findAcute(dr_transcription):\n",
    "    str1 = dr_transcription\n",
    "    str1 = str1.lower()\n",
    "    result = re.findall(r\"\\bacute\\b|\\bsevere\\b|\\bserious\\b|\\bgrave\\b|\\bcritical\\b\", str1)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find lower upper back\n",
    "def findLUB(dr_transcription):\n",
    "    str1 = dr_transcription\n",
    "    str1 = str1.lower()\n",
    "    result = re.findall(r\"\\blower\\b|\\bupper\\b|\\bback\\b|\\bmiddle\\b|\\bleft\\b|\\bright\\b\", str1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sensitivity', 'in', 'all', 'teeth']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n",
    "dr_trans = 'SENSITIVITY IN ALL TEETH'\n",
    "# Replaces escape character with space\n",
    "f = dr_trans.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "\ttemp = []\n",
    "\t\n",
    "\t# tokenize the sentence into words\n",
    "\tfor j in word_tokenize(i):\n",
    "\t\ttemp.append(j.lower())\n",
    "\n",
    "\tdata.append(temp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['left', 'knee', 'joint', 'pain', 'since', '3', 'years']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Removing html tags\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('[<,*?,>]')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "#Remove url tag\n",
    "def remove_url(text):\n",
    "    pattern = re.compile('http?//S+|www.S+')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "#Remove puncuation\n",
    "exclude = string.punctuation\n",
    "def remove_punct(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, '')\n",
    "        return text.translate(str.maketrans(' ',' ',exclude))\n",
    "\n",
    "\n",
    "#\n",
    "#Spelling correction\n",
    "def spell_correction(setense):\n",
    "    incorrect_text = 'pain in lower back x 1 year'\n",
    "    textblob = TextBlob(incorrect_text)\n",
    "    correct_text = textblob.correct()\n",
    "    return correct_text\n",
    "\n",
    "\n",
    "#Stop word Remover\n",
    "def stop_word_remover(sent):\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    words_without_stop_words = []\n",
    "    for word in tokens:\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        else:\n",
    "            words_without_stop_words.append(word)\n",
    "    return words_without_stop_words\n",
    "\n",
    "\n",
    "\n",
    "dr_trans = 'left knee joint pain since 3 years on and off'\n",
    "dr_trans = remove_html_tags(dr_trans)\n",
    "dr_trans = remove_url(dr_trans)\n",
    "dr_trans = remove_punct(dr_trans)\n",
    "dr_trans = stop_word_remover(dr_trans)\n",
    "dr_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords.add('since')\n",
    "#stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{4: 'd'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages = { 1: 'a', 2: 'b', 3:'c', 4: 'd'}\n",
    "m = 'd'\n",
    "value_43 = [{key:value} for key, value in ages.items() if value == m]\n",
    "value_43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files (x86)\\Python\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Program Files (x86)\\Python\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Program Files (x86)\\Python\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\asad\\nlp-engine-for-symptoms\\feature-extraction.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X50sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m drx\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X50sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m dr_transcriptionDList \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mfever\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mheadache\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mchills\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mabd\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X50sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m getDis(dr_transcriptionDList)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X50sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mret\u001b[39m\u001b[39m\"\u001b[39m,x)\n",
      "\u001b[1;32md:\\asad\\nlp-engine-for-symptoms\\feature-extraction.ipynb Cell 36\u001b[0m in \u001b[0;36mgetDis\u001b[1;34m(dr_transcriptionDList)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m dr_transcription:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     test_response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(test_api, json\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39malphabet\u001b[39m\u001b[39m\"\u001b[39m: t})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     response_data \u001b[39m=\u001b[39m test_response\u001b[39m.\u001b[39;49mjson()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m#print(response_data['responseValue'])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X50sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     mydict \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[39mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[39m.\u001b[39mmsg, e\u001b[39m.\u001b[39mdoc, e\u001b[39m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from tkinter import DOTBOX\n",
    "import requests\n",
    "def getDis(dr_transcriptionDList):\n",
    "    test_api =  'http://182.156.200.179:332/api/v1.0/Knowmed/getAllProblemList'\n",
    "    #payload = {\"alphabet\": \"a\"}\n",
    "    dr_transcription= dr_transcriptionDList\n",
    "    problemNames = {}\n",
    "    #nm = []\n",
    "    for t in dr_transcription:\n",
    "        test_response = requests.post(test_api, json={\"alphabet\": t})\n",
    "        response_data = test_response.json()\n",
    "        #print(response_data['responseValue'])\n",
    "        \n",
    "        \n",
    "        mydict = {}\n",
    "        \n",
    "        for i in range(len(response_data['responseValue'])):\n",
    "            #print(response_data['responseValue'][i]['problemName'])\n",
    "            #problemNames.append(response_data['responseValue'][i]['id'])\n",
    "            #nm.append(response_data['responseValue'][i]['problemName'])\n",
    "            #print(\"nm ->\",nm)\n",
    "            t = {response_data['responseValue'][i]['id']:response_data['responseValue'][i]['problemName'].lower()}\n",
    "            problemNames.update(t)\n",
    "            #print(\"t->\",t)\n",
    "            #x = [{id: id, 'problemName': problemName} for id, problemName in mydict.items() if problemName == search_value]\n",
    "        \n",
    "    drx = []\n",
    "    for dis in dr_transcription:\n",
    "            [drx.append({'id':key,'problemNames':value}) for key, value in problemNames.items() if value == dis]        \n",
    "    return drx\n",
    "dr_transcriptionDList = ['fever', 'headache', 'chills','abd']\n",
    "x = getDis(dr_transcriptionDList)\n",
    "print(\"ret\",x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 22, 'bodyOrgan': 'eye'}, {'id': 37, 'bodyOrgan': 'nose'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bodyOrganList = ['abdomen', 'abdomen (right hypochondium)', 'abdomen (right lumber)', 'abdomen (right eliac region)', 'abdomen (left hypochondium)', 'abdomen (left lumber)', 'abdomen (left eliac region)', 'abdomen (epigastric region)', 'abdomen (umbilical region)', 'abdomen (hypogastrium)', 'ankle', 'back', 'back (lower)', 'back (upper)', 'breast', 'buttock', 'calf', 'chest', 'ear', 'elbow', 'forehead', 'eye', 'face', 'finger', 'foot', 'hair', 'hand', 'head', 'heel', 'hip', 'knee', 'leg', 'lips', 'mouth', 'nail', 'neck', 'nose', 'palm', 'pelvis', 'shin', 'shoulder', 'skin', 'teeth', 'thigh', 'throat', 'thumb', 'toe', 'waist', 'wrist', 'full body', 'anywhere', 'vagina', 'penis', 'nipple', 'heart', 'kidney', 'lungs', 'liver', 'brain', 'bladder', 'stomach', 'intestines', 'uterus', 'blood', 'overy', 'testicles', 'limbs', 'spine', 'gollbladder', 'skull', 'gums', 'parotid gland', 'upper extremities', 'lower extremities', 'not specified', 'salivary glands', 'cartilagenous', 'multisystemic disorders', 'neurological disorder', 'bone', 'blood vessel', 'pancreas', 'adrenal glanel', 'lymphatic system', 'larynx', 'genital', 'cervix', 'bile duct', 'cns', 'prostate', 'respiratory system', 'skeletal system', 'renal system', 'cardivascular system', 'pitutiary gland', 'pneal gland', 'hypohealauous', 'parathyroid gland', 'thyroid gland', 'gastrointestinal system', 'anus', 'anal anal', 'oesophagus', 'rectum', 'cavernous sinus', 'tongue', 'paranasal sinus', 'hardplate', 'softplate', 'male reproductive system', 'female reproductive system', 'fallopian tubes', 'lower abdomen', 'adrenal glands', 'joint', 'multiple organ', 'gallbladder', 'muscle', 'muscle skeletal system', 'tissues', 'immune system  ', 'urinary tract infection', 'spinal cord', 'bone marrow', 'utreine corpus', 'testis', 'oral cavity', 'oropharynx', 'red blood cells', 'ans', 'hypothalamus', 'arteries', 'sole', 'trunk', 'labia majora', 'labia minora', 'glans penis', 'mouth angle', 'buccal mucosa', 'shaft penis', 'haematopoietic system', 'trachea', 'circulatory system', 'peripheral nervous system', 'depends on area of burn', 'vascular system', 'venous system', 'urinary bladder', 'mammary glands', 'reproductive system', 'urinary system']\n",
    "l1 = range(1,len(bodyOrganList))\n",
    "d = dict(zip(l1,bodyOrganList))\n",
    "matchedBodyOrgan = ['eye', 'nose']\n",
    "drOrgan = []\n",
    "for bo in matchedBodyOrgan:\n",
    "        [drOrgan.append({'id':key,'bodyOrgan':value}) for key, value in d.items() if value == bo]\n",
    "drOrgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 3198, 'problemNames': 'hypertension'}, {'id': 18651, 'problemNames': 'hypertension'}] {'id': 'problemNames'}\n"
     ]
    }
   ],
   "source": [
    "l1 = [{'id': 3198, 'problemNames': 'hypertension'}, {'id': 18651, 'problemNames': 'hypertension'}]\n",
    "l0 = dict(l1)\n",
    "print(l1,l0)\n",
    "l2 = [{'id': 9453, 'problemNames': 'diabetes mellitus type 2'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3198\n",
      "18651\n",
      "9453\n",
      "18651\n",
      "[{'id': 3198, 'problemNames': 'hypertension'}, {'id': 18651, 'problemNames': 'hypertension'}, {'id': 9453, 'problemNames': 'diabetes mellitus type 2'}, {'id': 18651, 'problemNames': 'hypertension'}]\n",
      "{'id': 18651, 'problemNames': 'hypertension'}\n"
     ]
    }
   ],
   "source": [
    "lx = [[{'id': 3198, 'problemNames': 'hypertension'}, {'id': 18651, 'problemNames': 'hypertension'}], [{'id': 9453, 'problemNames': 'diabetes mellitus type 2'}]]\n",
    "ly = [{'id': 3198, 'problemNames': 'hypertension'}, {'id': 18651, 'problemNames': 'hypertension'},{'id': 9453, 'problemNames': 'diabetes mellitus type 2'},{'id': 18651, 'problemNames': 'hypertension'}]\n",
    "ll = []\n",
    "for i in range(len(ly)):\n",
    "    print(ly[i]['id'])\n",
    "    ll.append(ly[i])\n",
    "print(ll)\n",
    "\n",
    "\n",
    "t ={k:v for element in ll for k,v in element.items()}\n",
    "print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\asad\\nlp-engine-for-symptoms\\feature-extraction.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m res \u001b[39m=\u001b[39m {}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m ll\u001b[39m.\u001b[39;49mitems():\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m value \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/asad/nlp-engine-for-symptoms/feature-extraction.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         res[key] \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "\n",
    "for key, value in ll.items():\n",
    "    if value not in res.values():\n",
    "        res[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x': 2020, 'y': 30}, {'x': 2021, 'y': 10}]\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "mylist = [{'x': 2020, 'y': 20}, {'x': 2020, 'y': 30}, {'x': 2021, 'y': 10}, {'x': 2021, 'y': 5}]\n",
    "\n",
    "mylist_unique = [{'x': key, 'y': max(item['y'] for item in values)}\n",
    "                 for key, values in groupby(mylist, lambda dct: dct['x'])]\n",
    "print(mylist_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 3198, 'problemNames': 'hypertension'},\n",
       " {'id': 9453, 'problemNames': 'diabetes mellitus type 2'},\n",
       " {'id': 18651, 'problemNames': 'hypertension'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my =[{'id': 3198, 'problemNames': 'hypertension'}, {'id': 18651, 'problemNames': 'hypertension'}, {'id': 9453, 'problemNames': 'diabetes mellitus type 2'}, {'id': 18651, 'problemNames': 'hypertension'}]\n",
    "[k for j, k in enumerate(my) if k not in my[j + 1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "173ae74ecd759d33659dc89cc0ace91dba90ddaa088b7a848a7f37d845ddcc5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
